{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Login Crab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "params = {'username': 'aaa','password':'password'}\n",
    "r = requests.post(\"http://www.pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "print(\"cookiese is set to: \")\n",
    "print(r.cookies.get_dict())\n",
    "print('Going to profile page...')\n",
    "r = requests.get(\"http://www.pythonscraping.com/pages/cookies/welcome.php\")\n",
    "\n",
    "display(HTML(r.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from IPython.core.display import display, HTML\n",
    "\n",
    "session = requests.Session()\n",
    "params = {'username': 'username','password':'password'}\n",
    "s = session.post(\"http://www.pythonscraping.com/pages/cookies/welcome.php\", params)\n",
    "print(\"cookiese is set to: \")\n",
    "print(s.cookies.get_dict())\n",
    "print('Going to profile page...')\n",
    "\n",
    "s = session.get(\"http://www.pythonscraping.com/pages/cookies/welcome.php\")\n",
    "\n",
    "display(HTML(s.text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from requests.auth import AuthBase\n",
    "from requests.auth import HTTPBasicAuth\n",
    "auth = HTTPBasicAuth('ryan', 'password')\n",
    "r = requests.post(url=\"http://www.pythonscraping.com/pages/auth/login.php\",auth=auth)\n",
    "print(r.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Catch Data and Create Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#此程式有使用google瀏覽器套件\n",
    "#不能在notebook上執行\n",
    "#目的為抓取網路店上ZenBook系列筆電的資料\n",
    "#結果的csv檔，是我用自己的電腦跑完再上傳到notebook上的\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import time\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests\n",
    "import csv\n",
    "import os\n",
    "import pymysql\n",
    "\n",
    "username = 'root'\n",
    "password = 'DB password'\n",
    "databaseName = 'DB name'\n",
    "\n",
    "\n",
    "chrome_option = Options()\n",
    "chrome_option.add_argument('--headless')#不希望出現視窗\n",
    "driver = webdriver.Chrome('chromedriver',chrome_options = chrome_option)#透過瀏覽器開啟\n",
    "\n",
    "driver.get('https://store.asus.com/tw/category/A15061')#Asus的官方網路店\n",
    "time.sleep(3)#等待網頁載入完成\n",
    "\n",
    "content = bs(driver.page_source,'html.parser')#透過BeautifulSoup開啟\n",
    "linkList = content.select('a.photo')#抓取各種筆電的連結\n",
    "urls = []\n",
    "for i in linkList:\n",
    "    urls.append(i.get('href'))#把連結放進urls\n",
    "#print(urls)\n",
    "driver.close()#關閉瀏覽器\n",
    "\n",
    "\n",
    "wantlist = ['型號','價格','CPU',\"GPU\",\"RAM\",\"資料儲存應用\",'保固']#這是我想找的資料，也是csv的第一列\n",
    "\n",
    "db = pymysql.connect('localhost',username,password,databaseName)\n",
    "cursor = db.cursor()\n",
    "\n",
    "tmp = []#用於暫存從網頁擷取下來的資料\n",
    "width = [0]*7#用於控制excel欄位寬度\n",
    "for url in urls: #透過urls，瀏覽每一台筆電的詳情\n",
    "    web = requests.get(\"https:\" + url)#請求網頁\n",
    "    content = bs(web.text,\"html.parser\")#用湯開啟\n",
    "    name = content.find('h1',id='pro_title').text#找到產品名稱\n",
    "    tmp.append(name)#放入暫存\n",
    "    #print(name)\n",
    "    price = content.find(\"span\",{'class':\"price\"}).text#找到價格\n",
    "    tmp.append(price)\n",
    "    #print(price)\n",
    "    item = content.select(\".css-spec-item\")#找到詳細資料標頭\n",
    "    itemData = content.select(\".css-spec-data\")#找到詳細資料\n",
    "    for i in range(len(item)):#將兩個list文字化(.text)\n",
    "        item[i] = item[i].text\n",
    "        itemData[i] = itemData[i].text.replace('\\n',\"\")\n",
    "    check = 0#用於確定資料完整\n",
    "    for i in range(len(item)):\n",
    "        if item[i] in wantlist:#篩選想要的資料\n",
    "            #print(item[i] + \":\" + itemData[i])\n",
    "            tmp.append(itemData[i])\n",
    "            check += 1\n",
    "    if check == 5:#如果資料完整\n",
    "        tmp[1] = int(tmp[1].replace(',',\"\"))\n",
    "        print(tmp)\n",
    "        cursor.execute('insert into laptop value(%s,%s,%s,%s,%s,%s,%s,null)',[tmp[i] for i in range(7)])\n",
    "    tmp.clear()#清空暫存\n",
    "db.commit()\n",
    "\n",
    "# crawler.py\n",
    "# 目前顯示的是「crawler.py」。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Link Database and Find by CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pymysql\n",
    "import pandas as pd\n",
    "username = 'root'\n",
    "password = 'hongwenwang'\n",
    "databaseName = \"comp\"\n",
    "# connectDB()\n",
    "db = pymysql.connect('localhost',username,password,databaseName)\n",
    "cursor = db.cursor()\n",
    "print(\"longin suceed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_activity(keyword=None):\n",
    "    msg=\"\"\n",
    "    if \"i7\" in keyword:\n",
    "        cursor.execute(\"SELECT * FROM laptop\")\n",
    "        result = cursor.fetchall()\n",
    "        for row in result:\n",
    "            for i in row:\n",
    "                msg += str(i)\n",
    "    return msg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask, escape, request, make_response, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def webhook():\n",
    "    json = request.get_json(silent=True,force=True)\n",
    "    print(json)\n",
    "    \n",
    "    CPU = None\n",
    "#     產品系列\n",
    "    if json['queryResult']['parameters']['Brand']!= '' :\n",
    "        Brand = json['queryResult']['parameters']['Brand']\n",
    "    \n",
    "#     GPU\n",
    "    if json['queryResult']['parameters']['GPU']!='':\n",
    "        GPU = json['queryResult']['parameters']['GPU']\n",
    "    \n",
    "#     價格\n",
    "    if json['queryResult']['parameters']['number']!='':\n",
    "        number = json['queryResult']['parameters']['number']\n",
    "    \n",
    "#     CPU\n",
    "    if json['queryResult']['parameters']['CPU']!='':\n",
    "        CPU = json['queryResult']['parameters']['CPU']\n",
    "        \n",
    "#     作業系統\n",
    "    if json['queryResult']['parameters']['OS']!='':\n",
    "        OS = json['queryResult']['parameters']['OS']\n",
    "    \n",
    "#     硬碟\n",
    "    if json['queryResult']['parameters']['Disk']!='':\n",
    "        disk = json['queryResult']['parameters']['Disk']\n",
    "        \n",
    "#     保固\n",
    "    if json['queryResult']['parameters']['any']!='':\n",
    "        warr = json['queryResult']['parameters']['any']\n",
    "\n",
    "#     特價表\n",
    "#     if json['queryResult']['parameters']['discount']!='':\n",
    "#         discount = json['queryResult']['parameters']['discount']\n",
    "        \n",
    "#     關注清單\n",
    "#     if json['queryResult']['parameters']['attention']!='':\n",
    "#         attention = json['queryResult']['parameters']['attention']\n",
    "        \n",
    "#     執行動作\n",
    "    if json['queryResult']['parameters']['action']!='':\n",
    "        action = json['queryResult']['parameters']['action']\n",
    "\n",
    "#     比較\n",
    "    if json['queryResult']['parameters']['compare']!='':\n",
    "        compare = json['queryResult']['parameters']['compare']\n",
    "    \n",
    "#     日期\n",
    "#     if json['queryResult']['parameters']['date']!='':\n",
    "#         date = parser.parse(json['queryResult']['parameters']['date'])\n",
    "#         start = date.date()\n",
    "#     elif json['queryResult']['parameters']['date-period']!='':\n",
    "#         start = parser.parse(json['queryResult']['parameters']['date-period']['startDate']).date()\n",
    "#         end = parser.parse(json['queryResult']['parameters']['date-period']['endDate']).date()\n",
    "    \n",
    "    res_message = {\"fulfillmentText\":get_keyword_activity(CPU)}\n",
    "    \n",
    "    print(res_message)\n",
    "    return make_response(jsonify(res_message))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BeautifulSoup find function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "html = requests.get(\"https://www.acer.com/ac/zh/TW/content/models/laptops\")# Acer 筆電官網\n",
    "soup = bs(html.text,\"lxml\")\n",
    "div = soup.find(\"div\",{\"class\",\"product-list\"}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = div.find_all(\"img\",{\"class\",\"centerImage zh-heightImg\"})#圖片\n",
    "#圖片處理\n",
    "imgLink=[]\n",
    "for row in img:\n",
    "    imgLink.append(row.get(\"src\").strip())\n",
    "# print(imgLink)\n",
    "len(imgLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = div.find_all(\"h2\")#型號\n",
    "#型號處理\n",
    "ID=[]\n",
    "for row in types:\n",
    "    ID.append(row.text.strip())\n",
    "len(ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = div.find_all(\"a\",{\"class\", \"zh-heightContImg\"})\n",
    "conLink = []\n",
    "for row in content:\n",
    "    conLink.append( \"https://www.acer.com/ac/\" + row.get(\"href\").strip())\n",
    "len(conLink)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page = requests.get(conLink[0])\n",
    "# time.sleep(3)\n",
    "soup = bs(html.text,\"lxml\")\n",
    "service = soup.find(\"div\",{\"id\",\"printableArea\"}) \n",
    "# print(soup.text)\n",
    "soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### More Infomation Catch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "\n",
    "html = requests.get(\"https://www.acer.com/ac/zh/TW/content/models/laptops\")# Acer 筆電官網\n",
    "soup = bs(html.text,\"lxml\")\n",
    "div = soup.find(\"div\",{\"class\",\"product-list\"}) #將網頁中 div 找出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = div.find_all(\"img\",{\"class\",\"centerImage zh-heightImg\"})#圖片\n",
    "#圖片處理\n",
    "imgLink=[]\n",
    "for row in img:\n",
    "    imgLink.append(row.get(\"src\").strip())\n",
    "print(imgLink)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "types = div.find_all(\"h2\")#型號\n",
    "#型號處理\n",
    "Type=[]\n",
    "for row in types:\n",
    "    Type.append(row.text.strip())\n",
    "print(Type[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "ul = div.find_all(\"ul\",{\"class\",\"text-m text-tertiary-dark align-center ctnTechSpecList\"})\n",
    "\n",
    "OS = []\n",
    "CPU = []\n",
    "View = []\n",
    "RAM = []\n",
    "Disk = []\n",
    "\n",
    "for row in ul:\n",
    "#     print(row)\n",
    "    fields = row.find_all(\"span\")#找到所有li\n",
    "    os = 1\n",
    "    cpu = 1\n",
    "    view = 1\n",
    "    ram = 1\n",
    "    disk = 1\n",
    "    for i in fields: # 有些筆電格是不一樣 li 不一致 所以找關鍵字\n",
    "        t = i.text.strip()\n",
    "        if \"Windows\" in t or \"Chrome\" in t: \n",
    "            OS.append(t)  # OS\n",
    "            os = 0\n",
    "        if \"核心\" in t or \"處理器\" in t:\n",
    "            CPU.append(t)  # CPU\n",
    "            cpu =0\n",
    "        if \"Graphics\" in t or \"NVIDIA\" in t:\n",
    "            View.append(t)  # viewCard\n",
    "            view = 0\n",
    "        if \"DDR\" in t :\n",
    "            RAM.append(t)  # RAM\n",
    "            ram =0\n",
    "        if \"SSD\" in t or \"HDD\" in t or \"快閃記憶體\" in t:\n",
    "            Disk.append(t)  # Disk\n",
    "            disk = 0\n",
    "    if os == 1:#那台電腦配備未列出時\n",
    "        OS.append(\"\")\n",
    "    if cpu == 1:\n",
    "        CPU.append(\"\")\n",
    "    if view == 1:\n",
    "        View.append(\"\")\n",
    "    if ram == 1:\n",
    "        RAM.append(\"\")\n",
    "    if disk == 1:\n",
    "        Disk.append(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#欄位標題\n",
    "row_title = [] \n",
    "row_title.append(\"型號\")\n",
    "row_title.append(\"圖片連結\")\n",
    "row_title.append(\"作業系統\")\n",
    "# row_title.append(\"價格\")\n",
    "row_title.append(\"顯示卡\")\n",
    "row_title.append(\"CPU\")\n",
    "row_title.append(\"RAM\")\n",
    "row_title.append(\"DISK\")\n",
    "#row_title.append(\"SSD\")\n",
    "#row_title.append(\"HDD\")\n",
    "# row_title.append(\"保固期\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook = pd.DataFrame({\n",
    "    row_title[0]:Type,\n",
    "    row_title[1]:imgLink,\n",
    "    row_title[2]:OS,\n",
    "    row_title[3]:View,\n",
    "    row_title[4]:CPU,\n",
    "    row_title[5]:RAM,\n",
    "    row_title[6]:Disk,\n",
    "})\n",
    "# print(csv.head())\n",
    "# notebook.to_csv(\"notebook.csv\",index = False,sep='\\t',encoding=\"utf-8\") # 用excel會有亂碼\n",
    "notebook.to_csv(\"notebook.csv\",index = False,sep=',',encoding=\"utf_8_sig\") #sep為分隔符號 用excel不會有亂碼"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Completely Crab Infomation and Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_keyword_activity(keyword = None):\n",
    "    \n",
    "    html = requests.get(\"http://www.oiac.fcu.edu.tw/zh-tw/alliances/2\")\n",
    "    soup = bs(html.text,\"lxml\")\n",
    "\n",
    "    table = soup.find(\"div\", {\"id\" : \"grids\"}) #將網頁中 div 找出\n",
    "\n",
    "    #欄位標題\n",
    "    row_title = [] \n",
    "    row_title.append(\"名稱\")\n",
    "    row_title.append(\"連結\")\n",
    "   \n",
    "    #抓取欄位值\n",
    "\n",
    "    fields = []\n",
    "    links = []\n",
    "    name = []\n",
    "    \n",
    "    for row in table.find_all(\"strong\"):\n",
    "        fields = row.find(\"a\")            #廠商找到連結和名稱\n",
    "        name.append(fields.text.strip())  #加到list\n",
    "        if \"http\" not in fields.get(\"href\"):#得到連結不完整\n",
    "            links.append(\"NULL\")\n",
    "        else:\n",
    "            links.append(fields.get(\"href\").replace(\"#http\",\"http\"))#得到連結，若連結是 #http 改為 http\n",
    "\n",
    "    company_df = pd.DataFrame({row_title[0]:name, row_title[1]:links})#建立 Dataframe\n",
    "\n",
    "    \n",
    "    msg = \"\"\n",
    "\n",
    "    if keyword != None and keyword != \"total\":\n",
    "        for i,name in enumerate(company_df[\"名稱\"]):\n",
    "            if keyword in name:   #判斷 keyword 是否包含在 name 中\n",
    "                if msg == \"\":\n",
    "                    msg = \"有找到公司!!!\\n\"\n",
    "                msg +=  company_df[\"名稱\"][i] + '\\n' + company_df[\"連結\"][i] + ' \\n'\n",
    "    elif keyword == \"total\":   # keyword 在 dialogflow 中是 total的 entity會傳全部\n",
    "        for indax,activity in company_df.iterrows():\n",
    "            msg += company_df[\"名稱\"][indax] + ' ' + company_df[\"連結\"][indax] + '\\n' \n",
    "    else:             # 沒有 keyword 時\n",
    "        msg = \"enter the company\"\n",
    "        return msg\n",
    "    msg.strip() \n",
    "    print(msg)\n",
    "    if msg == \"\":\n",
    "        msg = \"沒有這間公司\"\n",
    "        \n",
    "\n",
    "    return msg[:300] #保證不超過ngrok的大小限制"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from flask import Flask, escape, request, make_response, jsonify\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/')\n",
    "def hello_world():\n",
    "    return 'Hello, World!'\n",
    "\n",
    "@app.route('/', methods=['POST'])\n",
    "def webhook():\n",
    "    \n",
    "    json = request.get_json(silent=True,force=True)\n",
    "    print(json)\n",
    "    keyword = None\n",
    "    if json['queryResult']['parameters']['any']!='':  #得到在 dialogflow 為 any 的 entity\n",
    "        keyword = json['queryResult']['parameters']['any']\n",
    "    elif json['queryResult']['parameters']['total']!='':#得到在 dialogflow 為 total 的 entity\n",
    "        keyword = \"total\"  #將 keyword 設為 total\n",
    "    \n",
    "    res_message = {\"fulfillmentText\":get_keyword_activity(keyword=keyword)} #得到要回傳訊息\n",
    "    \n",
    "    return make_response(jsonify(res_message))#回傳"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need using dialogflow\n",
    "if __name__ == \"__main__\":\n",
    "    app.run(port=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Word Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(\"https://www.cw.com.tw/article/article.action?id=5071446\")\n",
    "soup = bs(r.text,\"lxml\")\n",
    "div = soup.find(\"div\",{'class':None,\"id\":None})\n",
    "\n",
    "st = div.text.strip()# 接字串\n",
    "\n",
    "st = st.replace(\"\\n\",\" \") #取代\\n\n",
    "st"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def group_words(s):\n",
    "    return re.findall('[\\u4e00-\\u9fff]|[a-zA-Z0-9]+',s) #中文英文分開\n",
    "words = group_words(st)\n",
    "print(words[:50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngram 出現次數\n",
    "def getNgramCount_zip(words,n):\n",
    "    ngram = list(zip(*[words[i:] for i in range(n)]))\n",
    "    output = dict()\n",
    "    for i in range(len(ngram)):\n",
    "        newNgram = \"\".join(ngram[i])\n",
    "        if newNgram in output:\n",
    "            output[newNgram] += 1\n",
    "        else:\n",
    "            output[newNgram] = 1\n",
    "    return output\n",
    "two_grams = getNgramCount_zip(words,2)\n",
    "\n",
    "# from collections import OrderedDict\n",
    "# order = OrderedDict(sorted(two_grams.items(),key=lambda t: t[1],reverse =True))\n",
    "# # printdict(order,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_ipython().magic(u'matplotlib inline')\n",
    "from matplotlib.font_manager import FontProperties\n",
    "myfont = FontProperties(fname=r'NotoSerifCJKtc-Black.otf')\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set(font=myfont.get_family())\n",
    "sns.set_style(\"whitegrid\",{\"font.sans-serif\":['Microsoft JhengHei']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 用ngram出現次數 產生文字雲 \n",
    "from wordcloud import WordCloud\n",
    "plt.figure()\n",
    "wordcloud = WordCloud(font_path='./data/NotoSerifCJKtc-Black.otf',max_words=50).generate_from_frequencies(two_grams)\n",
    "\n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install jieba\n",
    "\n",
    "#斷詞\n",
    "import jieba\n",
    "jieba.load_userdict('dict.txt.big')\n",
    "segment = list(jieba.cut(st))\n",
    "print(segment[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#接回字串\n",
    "seg_text = ' '.join(segment)\n",
    "print(seg_text[:100])"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
